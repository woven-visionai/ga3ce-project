<!DOCTYPE html>
<html lang="en">

<script src="bootstrap.js"></script>
<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script> 
<!---
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
--->
<script src="load-mathjax.js" async></script>


<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
<style type="text/css">
body {
    font-weight: 300;
    font-size: 17px;
    font-family: "Google Sans", "Noto Sans", Castoro, sans-serif;
    margin-left: auto;
    margin-right: auto;
    color: #555; /* <-- This is the gray-ish color */
}

@media screen and (min-width: 980px){
    body {
        width: 980px;
    }
}


h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #1468b7;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1 {
    text-align: center;
}
h2,h3 {
    text-align: left;
}

h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    font-weight: 600;
    margin: 16px 0px 4px 0px;
}

.paper-title {
    padding: 1px 0px 1px 0px;
}
.conf {
    padding: 1px 0px 1px 0px;
    text-align: center;
    padding: 1px 0px 1px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}

.move-down {
    margin-top:0.6cm;
}

.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.col-8{
width: 12.5%;
}

.author-row, .affil-row {
    font-size: 26px;
}

.author-row-new { 
    text-align: center; 
}

.author-row-new a {
    display: inline-block;
    font-size: 20px;
    padding: 4px;
}

.author-row-new sup {
    color: #313436;
    font-size: 12px;
}

.affiliations-new {
    font-size: 18px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 18px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 4px;
    margin-bottom: 10px;
	text-align: left;
}


video {
    display: block;
    margin: auto;
}


figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}

.paper-btn-coming-soon {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 15px;
  
  background-color: #838383;
  color: white !important;
  font-size: 20px;
  width: 100px;
  font-weight: 200;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.center {
  margin-left: 10.0%;
  margin-right: 10.0%;
}

.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 15px;
  
  background-color: #3e3131;
  color: white !important;
  font-size: 20px;
  width: 100px;
  font-weight: 200;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.paper-btn:hover {
    opacity: 0.85;
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 23px;
}

.topnav {
    background-color: #EEEEEE;
    overflow: hidden;
}

.topnav div {
    max-width: 1070px;
    margin: 0 auto;
}

.topnav a {
    display: inline-block;
    color: black;
    text-align: center;
    vertical-align: middle;
    padding: 16px 16px;
    text-decoration: none;
    font-size: 18px;
}

.topnav img {
    padding: 2px 0px;
    width: 100%;
    margin: 0.2em 0px 0.3em 0px;
    vertical-align: middle;
}

pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}

@media only screen and (max-width: 620px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 50%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}
* {
  box-sizing: border-box;
}

.column {
  text-align: center;
  float: left;
  width: 16.666%;
  padding: 5px;
}
.column3 {
  text-align: center;
  float: left;
  width: 33.333%;
  padding: 5px;
}
.column4 {
  text-align: center;
  float: left;
  width: 50%;
  padding: 5px;
}
.column5 {
  text-align: center;
  float: left;
  width: 20%;
  padding: 5px;
}
.column10 {
  text-align: center;
  float: left;
  width: 10%;
  padding: 5px;
}
.border-right {
    border-right: 1px solid black;
}
.border-bottom{
    border-bottom: 1px solid black;
}


.row-center {
    margin: 16px 0px 16px 0px;
    text-align: center;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
.img-fluid {
  max-width: 100%;
  height: auto;
}
.figure-img {
  margin-bottom: 0.5rem;
  line-height: 1;
}








.rounded-circle {
  border-radius: 50% !important;
}






/* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
@media screen and (max-width: 500px) {
  .column {
    width: 100%;
  }
}
@media screen and (max-width: 500px) {
  .column3 {
    width: 100%;
  }
}

</style>
<link rel="stylesheet" href="bootstrap-grid.css">
<link rel="stylesheet" href="simplegrid.css">

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title> GA3CE: Unconstrained 3D Gaze Estimation with Gaze-Aware 3D Context Encoding</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="Unconstrained 3D gaze estimation with egocentric spatial understanding"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:creator" content="">
        <meta name="twitter:title" content="GA3CE: Unconstrained 3D Gaze Estimation with Gaze-Aware 3D Context Encoding">
        <meta name="twitter:description" content="">
        <meta name="twitter:image" content="">
    </head>

<body>

<div class="container">
    <div class="paper-title">
    <h1> 
        GA3CE: Unconstrained 3D Gaze Estimation with Gaze-Aware 3D Context Encoding
    </h1>
    </div>
    
    <h2><div class="conf">CVPR 2025</div></h2>
    </br>

    <div id="authors">
        <center>
            <div class="author-row-new">
                <a href="https://www.linkedin.com/in/yuki-kawana-42b28526">Yuki Kawana</a>,
                <a href="http://shibashintaro.com/">Shintaro Shiba</a>,
                <a href="https://fusionk.github.io/quankong/">Quan Kong</a>,
                <a href="https://jp.linkedin.com/in/norimasa-kobori">Norimasa Kobori</a>
            </div>
        </center>
        <center>
        <div class="affiliations">
            <span class="author-block"><a href="https://woven.toyota/" style="vertical-align:middle"><img style="width:30px" src="img/woven.png"></a> Woven by Toyota</span>
        </div>



        </center>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="https://arxiv.org/abs/2505.10671">
                <span class="material-icons"> feed </span> 
                 arXiv
            </a>

            </div>
        </div>
    </div>

    <section id="teaser-image">
        <center>
            <figure>
                <video class="centered" width="100%" autoplay loop muted playsinline class="video-background " >
                    <source src="img/teaser.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>

            </figure>

        </center>
    </section>

    <section id="tldr"/>
        <h2>TL;DR</h2>
        <hr>
        <div class="flex-row">
            <p>
                We estimate 3D gaze in egocentric space based on explicit 3D understanding.
            </p>
        </div>
    </section>

    <section id="abstract"/>
        <h2>Abstract</h2>
        <hr>
        <div class="flex-row">
            <p>
                We propose a novel 3D gaze estimation approach that learns spatial relationships between the subject and objects in the scene, and outputs 3D gaze direction. Our method targets unconstrained settings, including cases where close-up views of the subject's eyes are unavailable, such as when the subject is distant or facing away. Previous approaches typically rely on either 2D appearance alone or incorporate limited spatial cues using depth maps in the non-learnable post-processing step. Estimating 3D gaze direction from 2D observations in these scenarios is challenging; variations in subject pose, scene layout, and gaze direction, combined with differing camera poses, yield diverse 2D appearances and 3D gaze directions even when targeting the same 3D scene. To address this issue, we propose GA3CE: Gaze-Aware 3D Context Encoding. Our method represents subject and scene using 3D poses and object positions, treating them as 3D context to learn spatial relationships in 3D space. Inspired by human vision, we align this context in an egocentric space, significantly reducing spatial complexity. Furthermore, we propose D$^3$ (direction-distance-decomposed) positional encoding to better capture the spatial relationship between 3D context and gaze direction in direction and distance space. Experiments demonstrate substantial improvements, reducing mean angle error by 13&#37;&ndash;37&#37; compared to leading baselines on benchmark datasets in single-frame settings.
            </p>
            <img src="img/overview.png" alt="overview" class='img-fluid'>
        </div>
    </section>

    <div class="section">
        <h2>Video</h2>
        <hr>

        <div class="row align-items-center">
            <video class="centered video-background" width="100%" controls playsinline>
                <source src="img/video.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>

            </div> 
        </div>
    </div>

    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>
@inproceedings{kawana2025ga3ce,
    title     = {GA3CE: Unconstrained 3D Gaze Estimation with Gaze-Aware 3D Context Encoding},
    author    = {Kawana, Yuki and Shiba, Shintaro and Kong, Quan and Kobori, Norimasa},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year      = {2025}
}
        </code></pre>
    </section>

    <!-- <section> 
        This webpage template was adapted from <a href='https://nv-tlabs.github.io/LION/'>here</a>.
    </section> -->

</div>

</body>
</html>
